{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L2: Information Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab you will implement and evaluate a simple system for information extraction. The task of the system is to read sentences and extract entity pairs of the form *x*&ndash;*y* where *x*&nbsp;is a person, *y*&nbsp;is an organisation, and *x* is the &lsquo;leader&rsquo; of&nbsp;*y*. Consider the following example sentence:\n",
    "\n",
    "<blockquote>\n",
    "Mr. Obama also selected Lisa Jackson to head the Environmental Protection Agency.\n",
    "</blockquote>\n",
    "\n",
    "From this sentence the system should extract the pair\n",
    "```\n",
    "(\"Lisa Jackson\", \"Environmental Protection Agency\")\n",
    "```\n",
    "\n",
    "The system will have to solve the following sub-tasks:\n",
    "* entity extraction &ndash; identifying mentions of person entities in text\n",
    "* relation extraction &ndash; identifying instances of the &lsquo;is-leader-of&rsquo; relation\n",
    "\n",
    "The data set for the lab consists of 62,010&nbsp;sentences from the [Groningen Meaning Bank](http://gmb.let.rug.nl) (release 2.2.0), an open corpus of English. To analyse the sentences you will use [spaCy](https://spacy.io/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first cell imports the Python module required for this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import tm2\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is contained in the following file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_file = \"gmb.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `tm2` module defines a function `read_data` that returns an iterator over the lines in a file. You should use this function to read the data for this lab. Use the optional argument `n` to restrict the iteration to the first few lines of the file. Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked assailants with grenades and automatic weapons attacked a wedding party in southeastern Turkey, killing 45 people and wounding at least six others.\n",
      "Turkish officials said the attack occurred Monday in the village of Bilge about 600 kilometers from Ankara.\n",
      "The wounded were taken to the hospital in the nearby city of Mardin.\n"
     ]
    }
   ],
   "source": [
    "def read_data(data_file, n=None):\n",
    "    with open(data_file) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if n and i >= n:\n",
    "                break\n",
    "            yield line.rstrip()\n",
    "\n",
    "for sentence in read_data(data_file, n=3):\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell imports spaCy and loads its English language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en', disable=['textcat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement the entity extraction part of your system, you do not need to do much, as you can use the full natural language processing power built into spaCy. The following code extracts the entities from the first 5&nbsp;sentences of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turkey\t13\t14\tGPE\n",
      "45\t16\t17\tCARDINAL\n",
      "at least six\t20\t23\tCARDINAL\n",
      "Turkish\t0\t1\tNORP\n",
      "Monday\t6\t7\tDATE\n",
      "Bilge\t11\t12\tORG\n",
      "about 600 kilometers\t12\t15\tQUANTITY\n",
      "Ankara\t16\t17\tGPE\n",
      "Mardin\t12\t13\tORG\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(nlp.pipe(read_data(data_file, n=5))):\n",
    "    for ent in doc.ents:\n",
    "        print(\"{}\\t{}\\t{}\\t{}\".format(ent.text, ent.start, ent.end, ent.label_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the [section about named entities](https://spacy.io/usage/linguistic-features#section-named-entities) from spaCy&rsquo;s documentation to get some background on this. (Please note that we are using version&nbsp;1 of the spaCy library, which means that there may be slight differences in the usage. At the time of writing, the current version&nbsp;2 is not yet stable and fast enough for this lab.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Extract relevant pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first problem that you will have to solve is to identify pairs of entities that are in the &lsquo;is-leader-of&rsquo; relation, as in the example above. There are many ways to do this, but for this lab it suffices to implement the strategy outlined in the section on [Relation Extraction](http://www.nltk.org/book/ch07.html#relation-extraction) in the book by Bird, Klein, and Loper (2009):\n",
    "\n",
    "* look for all triples of the form $(X, \\alpha, Y)$ where $X$ and $Y$ are named entities of type *person* and $\\alpha$ is the intervening text\n",
    "* write a regular expression to match just those instances of $\\alpha$ that express the &lsquo;is-leader-of&rsquo; relation\n",
    "\n",
    "You can restrict your attention to adjacent pairs of entities &ndash; that is, cases where $X$ precedes $Y$ and $\\alpha$ does not contain other named entities.\n",
    "\n",
    "Write a function `extract` that takes an analysed sentence (represented as a spaCy [`Doc`](https://spacy.io/api/doc) object) and yields pairs $(X, Y)$ of strings representing entity mentions predicted to be in the &lsquo;is-leader-of&rsquo; relation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "re_of = re.compile(r'.*(?:head|leader)')\n",
    "\n",
    "def extract(doc):\n",
    "    \"\"\"Extract relevant relation instances from the specified document.\n",
    "    \n",
    "    Args:\n",
    "        doc: The sentence as analysed by spaCy.\n",
    "    Yields:\n",
    "        Pairs of strings representing the extracted relation instances.\n",
    "    \"\"\"\n",
    "    for i in range(len(doc.ents) - 1):\n",
    "        ent1 = doc.ents[i]\n",
    "        ent2 = doc.ents[i+1]\n",
    "        #if ent1.label_ in ['PERSON', 'ORG'] and ent2.label_ in ['PERSON', 'ORG']:\n",
    "        if ent1.label_ in ['PERSON'] and ent2.label_ in ['ORG']:\n",
    "            alpha = doc[ent1.end:ent2.start]\n",
    "            if re_of.match(alpha.text):\n",
    "                yield [ent1.text, ent2.text]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell shows how your function is supposed to be used. The code prints out the extracted pairs for the \n",
    "first 1,000&nbsp;sentences in the data. It additionally numbers each pair with the identifier of the sentence (line number in the data file) which it was extracted from. Note that the sentence (line) numbering starts at index&nbsp;0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i, doc in enumerate(nlp.pipe(read_data(data_file, n=10))):\n",
    "    for person, org in extract(doc):\n",
    "        print(\"{}\\t{}\\t{}\".format(i, person, org))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you feel confident that your `extract` function does what it is supposed to do, execute the following cell to extract the entities from the full data set. Note that this will probably take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62009\n"
     ]
    }
   ],
   "source": [
    "extracted = set()\n",
    "for i, doc in enumerate(nlp.pipe(read_data(data_file))):\n",
    "    print(i, end='\\r')\n",
    "    for person, org in extract(doc):\n",
    "        extracted.add((i, person, org))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After executing the above cell, all extracted id-string-string triples are in the set `extracted`. The code in the next cell will print the first 10&nbsp;triples in this set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207\tRugova\tEuropean Union\n",
      "351\tJendayi Frazer\tSudan Liberation Army\n",
      "512\tAung San Suu Kyi\tthe National League for Democracy\n",
      "736\tViktor Yanukovych\tRussian Party\n",
      "802\tAsif Ali Zardari\tthe Pakistan People's Party\n",
      "1790\tKoizumi\tthe United Nations\n",
      "2297\tAbdul Aziz al-Hakim\tthe Supreme Council for the Islamic Revolution in Iraq\n",
      "3520\tPeres\tAmir Peretz\n",
      "4567\tBush\tthe U.S. Justice Department\n",
      "4753\tJunichiro Koizumi\tAPEC\n"
     ]
    }
   ],
   "source": [
    "for i, person, org in sorted(extracted)[:10]:\n",
    "    print(\"{}\\t{}\\t{}\".format(i, person, org))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Evaluate your system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now have an extractor, but how good is it? To help you answer this question, we provide you with a &lsquo;gold standard&rsquo; of entity pairs that your system should be able to extract. The following code loads them (again augmented with the relevant sentence id) from the file `gold.txt` and adds them to the set `gold`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gold_file = \"gold.txt\"\n",
    "\n",
    "gold = set()\n",
    "with open(gold_file) as fp:\n",
    "    for line in fp:\n",
    "        columns = line.rstrip().split('\\t')\n",
    "        gold.add((int(columns[0]), columns[1], columns[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code prints the 10&nbsp;first pairs from the gold standard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "802\tAli Zardari\tPakistan People 's Party\n",
      "2297\tAbdul Aziz al-Hakim\tSupreme Council\n",
      "4823\tSlavkov\tBulgarian National Olympic Committee\n",
      "7902\tMr. Hakim\tSupreme Council\n",
      "8206\tJ. Patrick Boyle\tAmerican Meat Institute\n",
      "8633\tAli Rodriguez\tPetroleos de Venezuela\n",
      "9004\tForeign Minister Joschka Fischer\tGreen Party\n",
      "11021\tKhalaf\tal-Qaida\n",
      "11259\tJoseph Domenech\tU.N. 's Food and Agricultural Organization\n",
      "13043\tDavid Petraeus\tU.S. Central Command\n"
     ]
    }
   ],
   "source": [
    "for i, person, org in sorted(gold)[:10]:\n",
    "    print(\"{}\\t{}\\t{}\".format(i, person, org))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task now is to write code that computes the precision, recall, and F1 measure of your extractor relative to the gold standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(reference, predicted):\n",
    "    \"\"\"Print out the precision, recall, and F1 for the id-entity-entity\n",
    "    triples in the set `predicted`, given the triples in the reference set.\n",
    "    \n",
    "    Args:\n",
    "        reference: The reference set of triples.\n",
    "        predicted: The set of predicted triples.\n",
    "    Returns:\n",
    "        Nothing, but prints out precision, recall, and F1.\n",
    "    \"\"\"\n",
    "    intersect = [triple for triple in predicted if triple in reference]\n",
    "    precision = len(intersect) / len(predicted)\n",
    "    recall = len(intersect) / len(reference)\n",
    "    harmonic_mean = 2 * (recall * precision) / (recall + precision)\n",
    "    \n",
    "    print('Precision: {:.2f}%'.format(precision*100))\n",
    "    print('Recall:    {:.2f}%'.format(recall*100))\n",
    "    print('F1 Score:  {:.2f}%'.format(harmonic_mean*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell shows how your function is intended to be used, as well as the suggested output format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 5.32%\n",
      "Recall:    10.87%\n",
      "F1 Score:  7.14%\n"
     ]
    }
   ],
   "source": [
    "evaluate(gold, extracted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Entity resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the results of your quantitative evaluation, you will realise that your extractor (probably) does a rather poor job in matching the gold standard. One reason for this is that the NLP preprocessing is not perfect (spaCy was not trained on the annotations in the Groningen Meaning Bank), and that the approach of using regular expressions for relation extraction is rather naive.\n",
    "\n",
    "Another reason however is that the current version of your system does not include a component for *entity resolution*. To give an example, your system does not realise that the strings `David Petraeus` and `General David Petraeus` refer to the same entity.\n",
    "\n",
    "While writing an entity resolver is beyond the scope of this assignment, we ask you to *simulate* such a resolver. More specifically, you should implement a function `normalise` that takes an entity mention (a string) as its input and rewrites it to the form used in the gold standard. While in some sense this is &lsquo;cheating&rsquo;, it allows you to assess the performance of a more realistic system.\n",
    "\n",
    "The following cell contains skeleton code for the `normalise` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gold_i = [i for i, a, b in gold]\n",
    "gold_a = [a for i, a, b in gold]\n",
    "gold_b = [b for i, a, b in gold]\n",
    "\n",
    "def normalise(text):\n",
    "    for ga in gold_a:\n",
    "        if text == ga:\n",
    "            return text\n",
    "        if text in ga or ga in text:\n",
    "            return ga\n",
    "    \n",
    "    for gb in gold_b:\n",
    "        if text == gb:\n",
    "            return text\n",
    "        if text in gb or gb in text:\n",
    "            return gb\n",
    "        \n",
    "    return text\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell shows how `normalise` is intended to be used. Each triple in the set `extracted` is transformed by feeding the two entity mentions into the `normalise` function. The normalised triples are then added to a new set `extracted_normalised`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extracted_normalised = set()\n",
    "for triple in extracted:\n",
    "    extracted_normalised.add((triple[0], normalise(triple[1]), normalise(triple[2])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To pass the assignment, you should add enough normalisation rules to `normalise` to achieve a recall of at least 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 28.72%\n",
      "Recall:    58.70%\n",
      "F1 Score:  38.57%\n"
     ]
    }
   ],
   "source": [
    "evaluate(gold, extracted_normalised)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: Limitations of the gold standard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each entity pair in the gold standard has been manually checked for correctness. However, there is no guarantee that the gold standard contains all relevant pairs &ndash; there are in fact many pairs that are missing from the gold standard. Your last task in this assignment is to find at least 5&nbsp;entity pairs in the data that are valid instances of the &lsquo;is-leader-of&rsquo; relation but are not contained in the gold standard.\n",
    "\n",
    "You can solve this task either by writing code or by manual work (inspecting the data file), or mix the two strategies. In any case, you should enter your pairs in the textbox below. Use the triple format shown above where for each pair you also specify the sentence id (line number in the data file) from which the instance was extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Used to find gold\n",
    "def dig_gold():\n",
    "    def get_sentences(data_file, line_numbers):\n",
    "        sentences = []\n",
    "        with open(data_file) as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if i in line_numbers:\n",
    "                    sentences.append([i, line.rstrip()])\n",
    "\n",
    "        return sentences\n",
    "\n",
    "    line_numbers = []\n",
    "    pruned_extracted = []\n",
    "    for i, a, b in extracted:\n",
    "        if i in gold_i:\n",
    "            continue\n",
    "        if i in line_numbers:\n",
    "            continue\n",
    "        line_numbers.append(i)\n",
    "        pruned_extracted.append([i, a, b])\n",
    "\n",
    "    sentences = get_sentences(data_file, line_numbers)\n",
    "    sentences.sort()\n",
    "\n",
    "    valid_sentences = []\n",
    "\n",
    "    checkpoint = 11000\n",
    "\n",
    "    for i, (line_nr, sentence) in enumerate(sentences):\n",
    "        if line_nr < checkpoint:\n",
    "            continue\n",
    "            pass\n",
    "        print(line_nr, sentence)\n",
    "        print(sorted(pruned_extracted)[i])\n",
    "        ans = input('Is good? [y/n]')\n",
    "        if ans == 'y':\n",
    "            valid_sentences.append(line_nr)      \n",
    "\n",
    "        if len(valid_sentences) >= 5:\n",
    "            break\n",
    "\n",
    "    for i, a, b in extracted:\n",
    "        if i in valid_sentences:\n",
    "            print(i, a, b, sep='\\t')\n",
    "# dig_gold()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "512\t    Aung San Suu Kyi\tthe National League for Democracy\n",
    "736     Viktor Yanukovych\tRussian Party\n",
    "10395\tYounus Khalis\t    Taleban\n",
    "11487\tAtta Mohammed\t    the Northern Alliance\n",
    "13637   Zardari             Pakistan People's Party"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we ask you to reflect on the limitations of the evaluation that you carried out in this lab and discuss the question: *How should systems for information extraction really be evaluated?*. Here are some starting points for your discussion.\n",
    "\n",
    "* How could one create a better gold standard for this task?\n",
    "* What do precision, recall, and F1 actually measure in this context?\n",
    "* What measures would be more suitable to evaluate this task?\n",
    "* What other ways of evaluating systems for information extraction can you think of?\n",
    "\n",
    "Submit your discussion as a short text (ca. 250&nbsp;words). When presenting your arguments, link back to your own results and experience from this lab, and to concepts you have learned in the lectures or in other parts of the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations of the evaluation\n",
    "### How could one create a better gold standard for this task?\n",
    "Relax the methods used to gather the triplet candidates. This would of course require more manual labor to prune incorrectly tagged triplets but would result in a larger more diverse gold standard. The gold standard used in this lab is very slim. Only 46 triplets from 62k lines of text. A \"ture\" gold standard probably contains a lot more than 46. This results in poor accuracy for our entity extractor when in reailty it would preform better. Recall however would have the opposite effect, preforming unrealistically well.\n",
    "\n",
    "### What do precision, recall, and F1 actually measure in this context?\n",
    "**Precision:** In percentage, from our predictions, how big proportion of them are correct. Predicting a lot of incorrect triplets will decrase the accuracy. Where precision fails to tell the whole story is how many correct triplets we missed. Making only one single prediction will result in 100% precision if it is correct.\n",
    "\n",
    "**Recall:** In percentage, how many of the golden ones did we predict. To complement precision recall can be used. Being too \"unrelaxed\" might give us a good precision but a bad recall. Being too relaxed might find all correct triplets and give a good recall, but also making including a lot of incorrect ones. \n",
    "\n",
    "**F1:** A measure that takes into account both precision and recall (2*p*r)/(p+r). This is the best of both worlds and is a meassure that can be used to keep a good balance. Keeping both precision and recall away from low percentages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the end of the assignment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
